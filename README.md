<div align="center">

<img src="assets/cover.png" alt="Cover" width="800">

<br><br>

# Visually-rich Document Understanding

### 2025 Samsung AI Challenge | On-Device Document Understanding System

<br>

[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.5.1-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org)
[![YOLO](https://img.shields.io/badge/YOLOv12-Ultralytics-00FFFF?style=for-the-badge&logo=yolo&logoColor=black)](https://github.com/ultralytics/ultralytics)
[![EasyOCR](https://img.shields.io/badge/EasyOCR-1.7.2-FF6F00?style=for-the-badge)](https://github.com/JaidedAI/EasyOCR)
[![CUDA](https://img.shields.io/badge/CUDA-12.1-76B900?style=for-the-badge&logo=nvidia&logoColor=white)](https://developer.nvidia.com/cuda-toolkit)
[![Optuna](https://img.shields.io/badge/Optuna-HPO-2496ED?style=for-the-badge)](https://optuna.org)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.7.1-F7931E?style=for-the-badge&logo=scikitlearn&logoColor=white)](https://scikit-learn.org)
[![License](https://img.shields.io/badge/Data-CC_BY_4.0-lightgrey?style=for-the-badge)](https://creativecommons.org/licenses/by/4.0/)

<br>

> PDF, PPTX, ì´ë¯¸ì§€ ë“± ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë¬¸ì„œì—ì„œ **ë ˆì´ì•„ì›ƒ íƒì§€**, **ì½ê¸° ìˆœì„œ ì˜ˆì¸¡**, **í…ìŠ¤íŠ¸ ì¶”ì¶œ(OCR)**ì„ í†µí•© ìˆ˜í–‰í•˜ëŠ” ê²½ëŸ‰ ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì´í•´ íŒŒì´í”„ë¼ì¸

</div>

---

## ğŸ† Competition Result

<table>
<tr>
<td width="65%" align="center">

<img src="assets/leaderboard.png" alt="Leaderboard" width="100%">

</td>
<td width="35%" align="center">

<img src="assets/award_ceremony.jpg" alt="Award Ceremony" width="100%">

<h3>ğŸ¥ˆ Excellence Award</h3>
<h4>(ìš°ìˆ˜ìƒ)</h4>
<p>Samsung AI Challenge 2025</p>

</td>
</tr>
</table>

<div align="center">

| ğŸ… Achievement | Details |
|:--------------:|:--------|
| **Private Score** | **0.45777** |
| **Final Rank** | ğŸ¥ˆ **2nd / 264 teams** (Top 0.8%) |
| **Prize** | ğŸ† **Excellence Award (ìš°ìˆ˜ìƒ, 500ë§Œì›)** |
| Fine-tuned mAP50 | **0.852** |
| Fine-tuned mAP50-95 | **0.701** |
| OCR Speed | Surya OCR ëŒ€ë¹„ **2.7x** |
| Total Model Size | **221 MB** (On-device deployable) |
| Training Time | ~4h (NVIDIA H200, 61 epochs) |

</div>

---

## ğŸ“‹ Overview

ë³¸ í”„ë¡œì íŠ¸ëŠ” **2025 ì‚¼ì„± AI ì±Œë¦°ì§€ Visually-rich Document Understanding** íŠ¸ë™ì— ì°¸ê°€í•˜ì—¬ ê°œë°œí•œ **On-device ë¬¸ì„œ ì´í•´ ì‹œìŠ¤í…œ**ì…ë‹ˆë‹¤.

ê¸°ì¡´ OCR ê¸°ìˆ ì€ ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì¶”ì¶œì— ë¨¸ë¬¼ëŸ¬, ë¬¸ì„œì˜ êµ¬ì¡°ì  ë ˆì´ì•„ì›ƒê³¼ ì½ê¸° íë¦„ì„ ì¶©ë¶„íˆ ë°˜ì˜í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ë³¸ ì‹œìŠ¤í…œì€ ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **ë„¤ ê°€ì§€ í•µì‹¬ ì „ëµ**ì„ ì„¤ê³„í•˜ì˜€ìŠµë‹ˆë‹¤:

1. ğŸ”„ **ì ì‘í˜• ì´ì¤‘ ëª¨ë¸ ì „ëµ** - ì„¸ë¡œí˜•/ê°€ë¡œí˜• ë¬¸ì„œì— ë”°ë¼ ìµœì í™”ëœ YOLO ëª¨ë¸ì„ ìë™ ì„ íƒ
2. ğŸ› ï¸ **ê·œì¹™ ê¸°ë°˜ í›„ì²˜ë¦¬** - ë°˜ë³µì  ì˜¤íƒ íŒ¨í„´(ì¤‘ë³µ ë°•ìŠ¤, Title ì˜¤ë¶„ë¥˜, Caption ì˜¤íƒ)ì„ êµì •
3. ğŸ“– **êµ¬ì¡° ê¸°ë°˜ ì½ê¸° ìˆœì„œ ì•Œê³ ë¦¬ì¦˜** - ì¸ê°„ì˜ ë…ì„œ ìŠµê´€ì„ ëª¨ë°©í•œ 3ê°€ì§€ ì „ëµ (ì±…/ì‹ ë¬¸/í¬ìŠ¤í„° ì½ê¸°)
4. âš¡ **ê²½ëŸ‰ OCR ìµœì í™”** - EasyOCR + 800 DPI ë Œë”ë§ìœ¼ë¡œ ì†ë„ì™€ ì •í™•ë„ì˜ ê· í˜• ë‹¬ì„±

---

## ğŸ”„ Pipeline Architecture

ì‹œìŠ¤í…œì€ **6ë‹¨ê³„ ìˆœì°¨ íŒŒì´í”„ë¼ì¸**ìœ¼ë¡œ êµ¬ì„±ë˜ë©°, Layout Detectionì˜ ì •í™•ë„ê°€ ì „ì²´ ì„±ëŠ¥ì˜ ìƒí•œì„ ê²°ì •í•©ë‹ˆë‹¤.

<p align="center">
  <img src="assets/pipeline.png" alt="Pipeline Architecture" width="100%">
</p>

| Stage | Module | Description | Key Tech |
|:-----:|--------|-------------|----------|
| 1 | **Input** | PDF, PPTX, JPG/PNG ë“± ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë¬¸ì„œ ì…ë ¥ | - |
| 2 | **Document Conversion** | ë¬¸ì„œë¥¼ ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¡œ ë³€í™˜. PDFëŠ” 800 DPI ë Œë”ë§, PPTXëŠ” LibreOffice -> PDF -> PNG | `pdf2image`, `Poppler` |
| 3 | **Layout Detection** | ë¬¸ì„œ ë°©í–¥(ì„¸ë¡œ/ê°€ë¡œ) íŒë³„ í›„ ìµœì  YOLO ëª¨ë¸ë¡œ 6ê°œ í´ë˜ìŠ¤ íƒì§€ | `YOLOv12-L`, `Optuna` |
| 4 | **Post-Processing** | NMS, Subtitle->Title ìŠ¹ê²©, Single Top-Title, Caption í•„í„°ë§ | Rule-based |
| 5 | **Reading Order** | ë¬¸ì„œ ìœ í˜•ë³„ Book / Newspaper / Poster ì „ëµìœ¼ë¡œ ì½ê¸° ìˆœì„œ ê²°ì • | `KMeans` |
| 6 | **OCR** | CRAFT í…ìŠ¤íŠ¸ íƒì§€ + EasyOCR(í•œ/ì˜) + 10ë‹¨ê³„ í…ìŠ¤íŠ¸ í´ë¦¬ë‹ | `EasyOCR`, `CRAFT` |

---

## ğŸ“Š Evaluation Metrics

Samsung AI Challenge ê³µì‹ í‰ê°€ëŠ” **ì„¸ ê°€ì§€ ëª¨ë“ˆì˜ ê°€ì¤‘í•©(Weighted Sum)**ìœ¼ë¡œ ìµœì¢… ì ìˆ˜ë¥¼ ì‚°ì¶œí•©ë‹ˆë‹¤.

### Final Score

$$S = 0.35 \cdot S_{\text{det}} + 0.35 \cdot S_{\text{ro}} + 0.30 \cdot S_{\text{ocr}}$$

ê° ëª¨ë“ˆì€ ìˆœì°¨ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì—°ê²°ë˜ë¯€ë¡œ, **ìƒìœ„ ë‹¨ê³„ì˜ ì˜¤ë¥˜ê°€ í•˜ìœ„ ë‹¨ê³„ ì„±ëŠ¥ì— ì§ì ‘ ì „íŒŒ(Error Propagation)**ë©ë‹ˆë‹¤. ë”°ë¼ì„œ Layout Detectionì˜ ì •í™•ë„ê°€ ì „ì²´ ì‹œìŠ¤í…œ ì„±ëŠ¥ì˜ **ìƒí•œ(Upper Bound)**ì„ ê²°ì •í•©ë‹ˆë‹¤.

### Module-wise Metrics

| Module | Weight | Metric | Description |
|--------|:------:|--------|-------------|
| **Layout Detection** | 35% | COCO mAP@[.5:.95] | ì˜ˆì¸¡ ë°•ìŠ¤ë¥¼ confidence ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„, IoU ì„ê³„ê°’ 0.50~0.95 (0.05 ê°„ê²©)ì—ì„œ ì¹´í…Œê³ ë¦¬ë³„ APë¥¼ ì‚°ì¶œí•˜ì—¬ ì „ì²´ í‰ê·  |
| **Reading Order** | 35% | Coverage-weighted NED | GTì™€ ì˜ˆì¸¡ì„ IoU â‰¥ 0.5 ê¸°ì¤€ 1:1 ë§¤ì¹­ í›„, ìˆœì„œ ë¬¸ìì—´ì˜ Normalized Edit Distance ê³„ì‚° |
| **OCR** | 30% | 1 - mean(NED) | ë§¤ì¹­ëœ í…ìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬ì˜ SequenceMatcher ê¸°ë°˜ NED í‰ê·  (ë¯¸ë§¤ì¹­ ì‹œ NED=1.0 íŒ¨ë„í‹°) |

### Sub-score Formulas

**Layout Detection** - COCO-style mAP@[.5:.95]

$$S_{\text{det}} = \frac{1}{|C| \cdot |T|} \sum_{c \in C} \sum_{\theta \in T} AP(c, \theta)$$

> IoU threshold $T = \{0.50, 0.55, \ldots, 0.95\}$, Category $C = \{$title, subtitle, text, table, image, equation$\}$

**Reading Order** - Coverage-weighted NED

$$S_{\text{ro}}^{(d)} = \left(1 - \text{NED}(\mathbf{g},\, \mathbf{p})\right) \times \frac{|\text{matched}|}{|\text{GT}_{\text{RO}}|}$$

> $\mathbf{g}$: GT ìˆœì„œ, $\mathbf{p}$: ì˜ˆì¸¡ ìˆœì„œ, Coverage í•­ì€ ë¯¸ë§¤ì¹­ ê°ì²´ì— ëŒ€í•œ íŒ¨ë„í‹°

**OCR** - 1 - mean(NED)

$$S_{\text{ocr}}^{(d)} = 1 - \frac{1}{N}\sum_{i=1}^{N} \text{NED}\!\left(t_i^{\text{gt}},\, t_i^{\text{pred}}\right)$$

> í…ìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬(title, subtitle, text) ëŒ€ìƒ, ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ NED = 1.0 (ìµœëŒ€ íŒ¨ë„í‹°)

---

## ğŸ¯ Core Strategies

### 1. Adaptive Dual-Model Strategy

ì„¸ë¡œí˜• ë¬¸ì„œ(ë…¼ë¬¸, ë³´ê³ ì„œ)ì™€ ê°€ë¡œí˜• ë¬¸ì„œ(PPT, í¬ìŠ¤í„°)ëŠ” ë ˆì´ì•„ì›ƒ êµ¬ì„±ì´ ê·¼ë³¸ì ìœ¼ë¡œ ìƒì´í•©ë‹ˆë‹¤. ì´ ë„ë©”ì¸ ê°„ê·¹ì„ í•´ì†Œí•˜ê¸° ìœ„í•´ **ë¬¸ì„œ ë°©í–¥ì— ë”°ë¼ ëª¨ë¸ì„ ìë™ ì„ íƒ**í•©ë‹ˆë‹¤.

<table>
<tr>
<td width="50%" align="center">

**ğŸ“„ Portrait (ì„¸ë¡œí˜•)**

<img src="assets/detection_portrait.png" alt="Portrait Detection" width="95%">

`width <= height` â†’ **YOLOv12-DocLayNet** (Base)

</td>
<td width="50%" align="center">

**ğŸ–¼ï¸ Landscape (ê°€ë¡œí˜•)**

<img src="assets/detection_landscape.png" alt="Landscape Detection" width="95%">

`width > height` or PPTX â†’ **YOLOv12 Fine-tuned** (V5.pt)

</td>
</tr>
</table>

ê° ëª¨ë¸ì€ **Optuna**ë¥¼ í™œìš©í•˜ì—¬ í´ë˜ìŠ¤ë³„ confidence thresholdë¥¼ ëŒ€íšŒ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë…ë¦½ ìµœì í™”í•˜ì˜€ìŠµë‹ˆë‹¤:

| Category | Base Model (ì„¸ë¡œí˜•) | Fine-tuned Model (ê°€ë¡œí˜•) |
|----------|:-------------------:|:-------------------------:|
| Title | 0.15 | 0.45 |
| Subtitle | 0.15 | 0.10 |
| Text | 0.10 | 0.10 |
| Table | 0.10 | 0.10 |
| Equation | 0.10 | 0.10 |
| Image | 0.10 | 0.22 |

### 2. Structure-based Reading Order

ì˜ë¯¸ë¡ ì  ë¶„ì„ ì—†ì´ **ë ˆì´ì•„ì›ƒ êµ¬ì¡°ë§Œìœ¼ë¡œ ì¸ê°„ì˜ ë…ì„œ íŒ¨í„´ì„ ëª¨ë°©**í•˜ëŠ” 3ê°€ì§€ ì½ê¸° ì „ëµì„ ì„¤ê³„í•˜ì˜€ìŠµë‹ˆë‹¤.

<table>
<tr>
<td width="33%" align="center">

**ğŸ“š Book-style**

<img src="assets/reading_book.png" alt="Book Reading" width="70%">

ë‹¨ì¼ ì»¬ëŸ¼ ë¬¸ì„œ<br>
yì¢Œí‘œ â†’ xì¢Œí‘œ ìˆœ ì •ë ¬<br>
y-tolerance ê¸°ë°˜ ë¼ì¸ ê·¸ë£¹í•‘

</td>
<td width="33%" align="center">

**ğŸ“° Newspaper-style**

<img src="assets/reading_newspaper.png" alt="Newspaper Reading" width="70%">

ë©€í‹° ì»¬ëŸ¼ ë¬¸ì„œ<br>
KMeans í´ëŸ¬ìŠ¤í„°ë§(k=2,3)<br>
ì»¬ëŸ¼ë³„ ìœ„â†’ì•„ë˜ ìˆœì„œ

</td>
<td width="33%" align="center">

**ğŸª§ Poster Scan-mode**

<img src="assets/reading_poster.png" alt="Poster Reading" width="95%">

ê°€ë¡œí˜• ë¬¸ì„œ(PPT, í¬ìŠ¤í„°)<br>
Subtitle í–‰ ê¸°ì¤€ ì˜ì—­ ë¶„í• <br>
ìˆ«ì ì ‘ë‘ì‚¬ ìë™ ë²ˆí˜¸ìˆœ ì •ë ¬

</td>
</tr>
</table>

- **ğŸ“š Book-style**: ëª¨ë“  ìš”ì†Œë¥¼ yì¢Œí‘œ â†’ xì¢Œí‘œ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³ , y-tolerance ê¸°ë°˜ìœ¼ë¡œ ë¼ì¸ì„ ê·¸ë£¹í•‘í•˜ì—¬ ê° ë¼ì¸ ë‚´ì—ì„œ ì¢Œâ†’ìš°ë¡œ ì½ìŠµë‹ˆë‹¤.
- **ğŸ“° Newspaper-style**: í˜ì´ì§€ ë„ˆë¹„ì˜ 65% ì´ìƒ í…ìŠ¤íŠ¸ ìŠ¤íŒ¬ì„ ê°ì§€í•˜ì—¬ ì˜ì—­ì„ ë¶„í• í•˜ê³ , ê° ì˜ì—­ì—ì„œ KMeans í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ì»¬ëŸ¼ì„ ìë™ ê°ì§€í•©ë‹ˆë‹¤.
- **ğŸª§ Poster Scan-mode**: Subtitle í–‰ì„ ê¸°ì¤€ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ì˜ì—­ ë¶„í• í•©ë‹ˆë‹¤. ê° Subtitle ì•„ë˜ì˜ ì„¸ë¡œ ìŠ¤íŠ¸ë¦½ì„ ë…ë¦½ì ìœ¼ë¡œ ì½ìœ¼ë©°, ìˆ«ì ì ‘ë‘ì‚¬("1. ì„œë¡ ")ê°€ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ë²ˆí˜¸ìˆœ ì •ë ¬í•©ë‹ˆë‹¤.

### 3. Rule-based Post-processing

ëª¨ë¸ì˜ ë°˜ë³µì  ì˜¤ë¥˜ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ **4ë‹¨ê³„ í›„ì²˜ë¦¬**ë¥¼ ìˆœì°¨ ì ìš©í•©ë‹ˆë‹¤:

| Rule | Description | Parameter | í•´ê²°í•˜ëŠ” ë¬¸ì œ |
|:----:|-------------|-----------|---------------|
| **R1. NMS** | ì‹ ë¢°ë„ ê¸°ì¤€ ì •ë ¬ í›„ ì¤‘ë³µ ë°•ìŠ¤ ì œê±° | IoU = 0.5 | ë™ì¼ ì˜ì—­ ë³µìˆ˜ íƒì§€ |
| **R2. Subtitle â†’ Title** | ë‚®ì€ ì‹ ë¢°ë„ì˜ Subtitleì„ Titleë¡œ ì¬ë¶„ë¥˜ | conf < 0.80 | Title-Subtitle ì˜¤ë¶„ë¥˜ |
| **R3. Single Top-Title** | ìµœìƒë‹¨-ì¢Œì¸¡ Titleë§Œ ìœ ì§€, ë‚˜ë¨¸ì§€ëŠ” Subtitleë¡œ ë³€ê²½ | í˜ì´ì§€ë‹¹ 1ê°œ | ë³µìˆ˜ Title íƒì§€ |
| **R4. Caption Filter** | "Figure X:", "Table X:", "ê·¸ë¦¼", "í‘œ" íŒ¨í„´ ë§¤ì¹­ í›„ ì œê±° | conf < 0.95 | Caption ì˜¤íƒì§€ |

### 4. OCR Text Extraction

| Component | Model | Role |
|-----------|-------|------|
| Text Detection | **CRAFT** (craft_mlt_25k.pth) | ë¬¸ì ì˜ì—­ ì¸ì‹ ê¸°ë°˜ í…ìŠ¤íŠ¸ ìœ„ì¹˜ íƒì§€ |
| Korean Recognition | **korean_g2.pth** | í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì¸ì‹ |
| English Recognition | **english_g2.pth** | ì˜ì–´ í…ìŠ¤íŠ¸ ì¸ì‹ |

OCR ì¶”ì¶œ í›„ **10ë‹¨ê³„ í…ìŠ¤íŠ¸ í´ë¦¬ë‹ íŒŒì´í”„ë¼ì¸**ì„ ì ìš©í•©ë‹ˆë‹¤:

> NFKC ì •ê·œí™” â†’ ì¤„ë°”ê¿ˆ í†µì¼ â†’ Math/Script íƒœê·¸ ì œê±° â†’ í•˜ì´í”ˆ-ì¤„ë°”ê¿ˆ ë³‘í•© â†’ HTML íƒœê·¸ ì œê±° â†’ ë¶ˆë¦¿/ë²ˆí˜¸ ì œê±° â†’ ì¸ë¼ì¸ ê¸°í˜¸ ì œê±° â†’ í•˜ì´í”ˆ ë¶„ë¦¬ ë‹¨ì–´ ë³‘í•© â†’ ì¤„ë°”ê¿ˆâ†’ê³µë°± â†’ ì—°ì† ê³µë°± ì •ë¦¬

---

## ğŸ“ Dataset

> í•™ìŠµ ë°ì´í„°ì…‹ì€ **ìì²´ êµ¬ì¶•**í•˜ì˜€ìœ¼ë©°, ëŒ€íšŒ ì œê³µ PPT, ìì²´ ì œì‘ í¬ìŠ¤í„°, ì™¸ë¶€ ìˆ˜ì§‘ ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ êµ¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.

| í•­ëª© | ë‚´ìš© |
|:----:|------|
| **ë°ì´í„° ì†ŒìŠ¤** | NeurIPS 2024 í•™ìˆ  í¬ìŠ¤í„° |
| **êµ¬ì„±** | ëŒ€íšŒ ì œê³µ PPT + ìì²´ ì œì‘ í¬ìŠ¤í„° 21ì¥ + ì™¸ë¶€ ìˆ˜ì§‘ 79ì¥ |
| **ì´ ì´ë¯¸ì§€** | 100ì¥ |
| **ì–´ë…¸í…Œì´ì…˜ ë„êµ¬** | Roboflow |
| **ì–´ë…¸í…Œì´ì…˜ í˜•ì‹** | YOLO Detection Format (bbox + class) |
| **ì´ë¯¸ì§€ í•´ìƒë„** | 1123Ã—794 ~ 5120Ã—2880 (ê°€ë³€) |
| **ë¼ì´ì„ ìŠ¤** | CC BY 4.0 |
| **ë°ì´í„° ë¶„í• ** | Train 90ì¥ (90%) / Validation 10ì¥ (10%) |

### Detection Classes (6 Classes)

| Class | YOLO ID | Description | Example |
|:-----:|:-------:|-------------|---------|
| `title` | 5 | ë¬¸ì„œ ì œëª© | ë…¼ë¬¸/í¬ìŠ¤í„°ì˜ ë©”ì¸ ì œëª© |
| `subtitle` | 2 | ì„¹ì…˜ í—¤ë” | "1. Introduction", "Methods" |
| `text` | 4 | ë³¸ë¬¸ í…ìŠ¤íŠ¸ | ë‹¨ë½, ì„¤ëª…ë¬¸ |
| `table` | 3 | í‘œ | ì‹¤í—˜ ê²°ê³¼ í‘œ, ë°ì´í„° í…Œì´ë¸” |
| `image` | 1 | ì´ë¯¸ì§€/ê·¸ë¦¼ | ë‹¤ì´ì–´ê·¸ë¨, ì‚¬ì§„, ê·¸ë˜í”„ |
| `equation` | 0 | ìˆ˜ì‹ | ìˆ˜í•™ ê³µì‹, ë°©ì •ì‹ |

---

## ğŸš€ Training & Results

### Fine-tuning Configuration

| Parameter | Value | Rationale |
|-----------|:-----:|-----------|
| Base Model | YOLOv12-Large (DocLayNet pretrained) | ë¬¸ì„œ ë„ë©”ì¸ ì‚¬ì „ì§€ì‹ í™œìš© |
| Input Size | 1024 Ã— 1024 | ê³ í•´ìƒë„ ìš”ì†Œ íƒì§€ |
| Batch Size | 16 | GPU ë©”ëª¨ë¦¬ ìµœì  í™œìš© |
| Max Epochs | 300 (Early Stop at 61) | Patience=50ìœ¼ë¡œ ê³¼ì í•© ë°©ì§€ |
| LR Schedule | Cosine Annealing (0.01 â†’ 0.0001) | ì•ˆì •ì  ìˆ˜ë ´ |
| Augmentation | Mosaic 0.2, Scale Â±20%, Translation Â±5% | ë¬¸ì„œ êµ¬ì¡° ë³´ì¡´ ë³´ìˆ˜ì  ì¦ê°• |
| Flip / Rotation | **OFF** | í…ìŠ¤íŠ¸ ë°©í–¥/ì½ê¸° ìˆœì„œ ë³´ì¡´ |

### Performance

<p align="center">
  <img src="assets/performance_results.png" alt="Performance Results" width="100%">
</p>

**Module-wise Ablation Study:**

| Step | Setting | Public Score | Improvement |
|:----:|---------|:-----------:|:-----------:|
| 1 | Baseline (Surya OCR) | 0.1168 | - |
| 2 | + YOLOv12-DocLayNet | 0.1515 | +29.7% |
| 3 | + Rule-based Post-processing | 0.2243 | +48.0% |
| 4 | + Fine-tuning (V5.pt) | 0.2828 | +26.1% |
| 5 | + Optuna Threshold | 0.3285 | +16.2% |
| 6 | + Reading Order Strategies | 0.4320 | +31.5% |
| **7** | **+ EasyOCR + Text Cleaning** | **0.4577** | **+5.9%** |

---

## ğŸ§° Tech Stack

### Frameworks & Libraries

| Category | Tool | Version | Role |
|:--------:|------|:-------:|------|
| **Deep Learning** | PyTorch | 2.5.1+cu121 | ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ |
| **Detection** | Ultralytics (YOLOv12) | 8.3.185 | ë¬¸ì„œ ë ˆì´ì•„ì›ƒ íƒì§€ |
| **OCR** | EasyOCR | 1.7.2 | í•œ/ì˜ ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ ì¸ì‹ |
| **Text Detection** | CRAFT | - | ë¬¸ì ì˜ì—­ íƒì§€ |
| **Clustering** | scikit-learn | 1.7.1 | KMeans ë©€í‹°ì»¬ëŸ¼ ê°ì§€ |
| **Optimization** | Optuna | - | ì‹ ë¢°ë„ ì„ê³„ê°’ ìë™ ìµœì í™” |
| **Document** | pdf2image | 1.17.0 | PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ |
| **Image** | Pillow | 11.3.0 | ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì‹œê°í™” |
| **Data** | pandas | 2.3.2 | CSV ë°ì´í„° í•¸ë“¤ë§ |
| **Numeric** | NumPy | 2.2.6 | ìˆ˜ì¹˜ ì—°ì‚° |
| **Annotation** | Roboflow | - | ë°ì´í„°ì…‹ ì–´ë…¸í…Œì´ì…˜ |

### Training Environment

| Item | Specification |
|:----:|---------------|
| GPU | NVIDIA HGX H200 |
| CUDA | 12.1 |
| Training Time | ~4 hours (61 epochs, Early Stopped from 300) |
| Total Model Size | 221 MB (On-device deployable) |

---

## âš¡ Quick Start

### 1. Environment Setup

```bash
pip install -r requirements.txt
```

### 2. Training (Fine-tuning)

```bash
python scripts/train.py
```

ì£¼ìš” í•™ìŠµ ì„¤ì •ì€ `scripts/train.py` ìƒë‹¨ì˜ CONFIG ë¸”ë¡ì—ì„œ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- `SPLIT_RATIOS`: ë°ì´í„° ë¶„í•  ë¹„ìœ¨ (ê¸°ë³¸: 90/10/0)
- `EPOCHS`, `BATCH`, `IMGSZ`: í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
- `MOSAIC`, `SCALE`, `TRANSLATE`: ë°ì´í„° ì¦ê°• ì„¤ì •

### 3. Inference

```bash
python scripts/inference.py
```

ê²°ê³¼ëŠ” `outputs/submission.csv`ë¡œ ì €ì¥ë©ë‹ˆë‹¤. ì¶œë ¥ í˜•ì‹:

```csv
ID, category_type, confidence_score, order, text, bbox
doc1, title, 0.95, 0, "Attention Is All You Need", "10, 20, 500, 80"
doc1, text, 0.88, 1, "We propose a new simple network...", "10, 100, 500, 300"
```

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ assets/                     # README ì´ë¯¸ì§€ ë¦¬ì†ŒìŠ¤
â”‚
â”œâ”€â”€ configs/                    # ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ data.yaml               #   YOLO ë°ì´í„°ì…‹ ì„¤ì •
â”‚   â”œâ”€â”€ dataset_info.txt        #   ë°ì´í„°ì…‹ ì„¤ëª…
â”‚   â””â”€â”€ roboflow_info.txt       #   Roboflow ì–´ë…¸í…Œì´ì…˜ ì •ë³´
â”‚
â”œâ”€â”€ data/                       # ë°ì´í„°ì…‹ (ìì²´ êµ¬ì¶•)
â”‚   â”œâ”€â”€ raw/                    #   ì›ë³¸ ë°ì´í„° (100 í¬ìŠ¤í„° ì´ë¯¸ì§€ + YOLO ë¼ë²¨)
â”‚   â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â””â”€â”€ labels/
â”‚   â”œâ”€â”€ visualized/             #   BBox ì‹œê°í™” ìƒ˜í”Œ
â”‚   â”œâ”€â”€ dataset_metadata.csv    #   ë°ì´í„°ì…‹ ë©”íƒ€ ì •ë³´
â”‚   â””â”€â”€ data_license.csv        #   ë°ì´í„° ë¼ì´ì„ ìŠ¤
â”‚
â”œâ”€â”€ models/                     # ëª¨ë¸ ê°€ì¤‘ì¹˜
â”‚   â”œâ”€â”€ pretrained/             #   ì‚¬ì „í•™ìŠµ ëª¨ë¸ (4ê°œ)
â”‚   â”‚   â”œâ”€â”€ yolov12l-doclaynet.pt
â”‚   â”‚   â”œâ”€â”€ craft_mlt_25k.pth
â”‚   â”‚   â”œâ”€â”€ english_g2.pth
â”‚   â”‚   â””â”€â”€ korean_g2.pth
â”‚   â””â”€â”€ finetuned/              #   íŒŒì¸íŠœë‹ ëª¨ë¸ (3ê°œ)
â”‚       â”œâ”€â”€ V5.pt               #   ìµœì¢… ì œì¶œ ëª¨ë¸ (mAP50=0.852)
â”‚       â”œâ”€â”€ best.pt
â”‚       â””â”€â”€ last.pt
â”‚
â”œâ”€â”€ scripts/                    # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ train.py                #   í•™ìŠµ (YOLOv12 Fine-tuning)
â”‚   â”œâ”€â”€ inference.py            #   ì¶”ë¡  (Detection + OCR + Reading Order)
â”‚   â”œâ”€â”€ generate_figures.py     #   README ì´ë¯¸ì§€ ìƒì„±
â”‚   â””â”€â”€ draw_pipeline.py        #   ë…¼ë¬¸ìš© íŒŒì´í”„ë¼ì¸ ì‹œê°í™”
â”‚
â”œâ”€â”€ experiments/                # í•™ìŠµ ì‹¤í—˜ ë¡œê·¸
â”‚   â””â”€â”€ train_6cls_1024/        #   ë©”ì¸ ì‹¤í—˜ ê²°ê³¼
â”‚
â”œâ”€â”€ docs/                       # ë¬¸ì„œ
â”‚   â”œâ”€â”€ paper_draft.md          #   ê¸°ìˆ  ë…¼ë¬¸ ì´ˆì•ˆ
â”‚   â”œâ”€â”€ overleaf_paper/         #   LaTeX ë…¼ë¬¸ ì†ŒìŠ¤
â”‚   â””â”€â”€ solution_report.pdf     #   ì†”ë£¨ì…˜ ë³´ê³ ì„œ
â”‚
â”œâ”€â”€ requirements.txt            # Python ì˜ì¡´ì„±
â””â”€â”€ .gitignore
```

---

## ğŸ“„ Technical Paper

> ë³¸ í”„ë¡œì íŠ¸ì˜ ì „ì²´ ë°©ë²•ë¡ , ì‹¤í—˜ ì„¤ê³„, ê²°ê³¼ ë¶„ì„ì„ ì •ë¦¬í•œ ê¸°ìˆ  ë…¼ë¬¸ì…ë‹ˆë‹¤.

<details>
<summary><b>ğŸ“– Click to view Technical Paper (9 pages)</b></summary>

<br>

<div align="center">

<img src="assets/paper_page-1.png" alt="Paper Page 1" width="700">

---

<img src="assets/paper_page-2.png" alt="Paper Page 2" width="700">

---

<img src="assets/paper_page-3.png" alt="Paper Page 3" width="700">

---

<img src="assets/paper_page-4.png" alt="Paper Page 4" width="700">

---

<img src="assets/paper_page-5.png" alt="Paper Page 5" width="700">

---

<img src="assets/paper_page-6.png" alt="Paper Page 6" width="700">

---

<img src="assets/paper_page-7.png" alt="Paper Page 7" width="700">

---

<img src="assets/paper_page-8.png" alt="Paper Page 8" width="700">

---

<img src="assets/paper_page-9.png" alt="Paper Page 9" width="700">

</div>

</details>

---

## ğŸ“œ Model Licenses

| Model | License | Source |
|-------|:-------:|--------|
| YOLOv12 (Ultralytics) | AGPL-3.0 | [ultralytics/ultralytics](https://github.com/ultralytics/ultralytics) |
| YOLOv12-DocLayNet | AGPL-3.0 | [hantian/yolo-doclaynet](https://huggingface.co/hantian/yolo-doclaynet) |
| CRAFT (Text Detection) | Apache-2.0 | [clovaai/CRAFT-pytorch](https://github.com/clovaai/CRAFT-pytorch) |
| EasyOCR (Korean/English) | Apache-2.0 | [JaidedAI/EasyOCR](https://github.com/JaidedAI/EasyOCR) |
| Fine-tuning Dataset | CC BY 4.0 | Self-built (Roboflow annotated) |

---

## ğŸ“š References

1. Ultralytics. *YOLOv12: Real-time Object Detection.* AGPL-3.0. https://github.com/ultralytics/ultralytics
2. B. Pfitzmann et al. *DocLayNet: A Large Human-Annotated Dataset for Document-Layout Segmentation.* KDD, 2022.
3. Y. Baek et al. *Character Region Awareness for Text Detection.* CVPR, 2019.
4. JaidedAI. *EasyOCR: Ready-to-use OCR with 80+ Supported Languages.* Apache-2.0. https://github.com/JaidedAI/EasyOCR
5. T. Akiba et al. *Optuna: A Next-generation Hyperparameter Optimization Framework.* KDD, 2019.

---

<div align="center">

### ğŸ† Samsung AI Challenge 2025 - Excellence Award ğŸ†

[![GitHub](https://img.shields.io/badge/GitHub-jinnwoook-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/jinnwoook)

</div>
